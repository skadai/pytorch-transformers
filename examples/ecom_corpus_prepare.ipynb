{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ecom_senti语料准备\n",
    "\n",
    "##  特征词/情感词 阅读理解语料生成\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备相关文件\n",
    "\n",
    "1. 为本次任务指定文件夹, 文件夹名一般就是<task_name>, 后面模型训练和评估会用到\n",
    "2. 准备原始标注语料 general.json, 可以通过nlptools dump_dataset直接导出, 上传到步骤1目录\n",
    "3. 从nlp_label数据库下载当前品类的相关排除词文件 excludes.csv，上传到步骤1目录\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T09:33:53.255919Z",
     "start_time": "2019-09-12T09:33:53.232881Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "task_name = 'ecom_912'\n",
    "test_dir = f'/data/projects/bert_pytorch/{task_name}'   \n",
    "corpus_path = f'/data/projects/bert_pytorch/{task_name}/general.json'  # general.json 为当前训练任务对应的标注json line文件\n",
    "excludes_path = f'/data/projects/bert_pytorch/{task_name}/excludes.csv' # 排除词路径\n",
    "\n",
    "# 在SUBTYPE.json文件内写入当前品类的subtype说明\n",
    "category_name = 'general'\n",
    "trans_sub = json.load(open('SUBTYPE.json'))[category_name]\n",
    "trans_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按照各个subtype提取正样本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T09:34:50.258955Z",
     "start_time": "2019-09-12T09:34:46.765099Z"
    }
   },
   "outputs": [],
   "source": [
    "for subtype in trans_sub:\n",
    "    print(f'为{subtype}提取正样本')\n",
    "    note = subtype.replace(' ', '_').replace('/','.')\n",
    "    command = f'./extract_subtype_ecom.sh {note} bert_pytorch/{task_name}'\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据各个subtype提取负样本\n",
    "\n",
    "#### 从混淆subtype寻找负样本\n",
    "1. 同属于一个type的其他subtype样本,\n",
    "2. 不属于一个type的其他subtype容易混淆的样本\n",
    "3. 其他subtype的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T09:37:47.403787Z",
     "start_time": "2019-09-12T09:37:06.390124Z"
    }
   },
   "outputs": [],
   "source": [
    "# nega_sample_dict 定义了提取负样本的优先选择次序, 如对于 Logistic Service, \n",
    "# 优先从较容易混淆的 Shop/Customer Service中选择负样本, 便于模型能学习到两者的差异\n",
    "# 如果没为SUBTYPE指定其优先选择负样本subtype, 则会从相同type不同subtype中选取\n",
    "nega_sample_dict = {\n",
    "    'Wrong Delivery': ['Logistics Fee', 'Promotion'],\n",
    "    'Logistics Package': [\n",
    "        \"Package Cleanliness\",\n",
    "        \"Package Design\",\n",
    "        \"Package General\",\n",
    "        \"Package Integrity\",\n",
    "        \"Package Material\",\n",
    "        \"Package Printing\"\n",
    "    ],\n",
    "    'Logistics Service': ['Shop/Customer Service'],\n",
    "}\n",
    "\n",
    "# TODO: 此处需要整合到SUBTYPE.json文件\n",
    "# general_subtypes = [\n",
    "#      (\"Product\",\"Fat Granule\", \"脂肪粒\"  ),\n",
    "#      (\"Product\", \"Greasy\", \"油腻\"),\n",
    "#      (\"Product\", \"Irritation\", \"刺激\"),\n",
    "#      (\"Product\", \"Moisturization\", \"保湿\"),\n",
    "#      (\"Product\", \"Smell\", \"气味\"),\n",
    "#      (\"Product\", \"Whitening\", \"肤色改善\")]\n",
    "general_subtypes = [\n",
    "    ('Branding', 'Brand Equity', '品牌资产'),\n",
    "    ('Branding', 'Loyalty', '品牌忠诚度'),\n",
    "    ('Branding', 'New User', '品牌新用户'),\n",
    "    ('Branding', 'WOM', '品牌口碑'),\n",
    "    ('Authenticity', 'Fake Concern', '假货'),\n",
    "    ('Inventory', 'Inventory', '库存'),\n",
    "    ('Inventory', 'Expiration Date', '保质期'),\n",
    "    ('Logistics', 'Logistics Speed', '快递送货速度'),\n",
    "    ('Logistics', 'Pick-up Speed', '快递发货速度'),\n",
    "    ('Logistics', 'Wrong Delivery', '快递错发漏发'),\n",
    "    ('Logistics', 'Logistics Fee', '快递费用'),\n",
    "    ('Logistics', 'Logistics Service', '快递服务'),\n",
    "    ('Logistics', 'Logistics Company',  '快递公司'),\n",
    "    ('Logistics', 'Logistics Package', '快递包装'),\n",
    "    ('Logistics', 'Logistics Damage', '快递破损'),\n",
    "    ('Package', 'Package Cleanliness', '包装清洁度'),\n",
    "    ('Package', 'Package Design',  '包装设计'),\n",
    "    ('Package', 'Package Integrity', '包装完整度'),\n",
    "    ('Package', 'Package Material', '包装材质'),\n",
    "    ('Package', 'Package Printing', '包装印刷'),\n",
    "    ('Package', 'Package General',  '包装概览'),\n",
    "    ('Price', 'Price Satisfaction', '价格满意度'),\n",
    "    ('Price', 'Price Sensitivity', '价格敏感度'),\n",
    "    ('Promotion', 'Promotion', '促销'),\n",
    "    ('Service', 'Shop/Customer Service', '店铺或客服服务'),\n",
    "    ('Service', 'Return Exchange', '退换货服务')\n",
    "]\n",
    "\n",
    "for dirname in os.listdir(test_dir):\n",
    "    if os.path.isdir(os.path.join(test_dir, dirname)):\n",
    "        subtype = dirname.replace('.','/').replace('_',' ')\n",
    "        level_1 = list(filter(lambda x:x[1]== subtype,  general_subtypes))[0][0]\n",
    "        print('寻找负样本', subtype, level_1)\n",
    "        \n",
    "        samples = [i[1] for i in filter(lambda x: x[0]==level_1 and x[1]!= subtype,  general_subtypes)] + nega_sample_dict.get(subtype,[])\n",
    "        print(samples)\n",
    "        \n",
    "        nega_file_path = os.path.join(test_dir, dirname, 'nega_samples.json')\n",
    "        command = f'touch {nega_file_path}'  # 为没有排除词的subtype建立文件进行占位\n",
    "        os.system(command)\n",
    "        for s in samples:\n",
    "            command = f'grep -v \"{subtype}\" {corpus_path}| grep \"{s}\" >> {nega_file_path}'\n",
    "            !{command}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据排除词寻找各个subtype的负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T09:38:09.040246Z",
     "start_time": "2019-09-12T09:38:08.266327Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_exclude = pd.read_csv(excludes_path)\n",
    "df_exclude = df_exclude[df_exclude.aspect_subtype.isin(trans_sub)]\n",
    "df_exclude.groupby('aspect_subtype').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T09:39:09.094864Z",
     "start_time": "2019-09-12T09:38:17.276057Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for dirname in os.listdir(test_dir):\n",
    "    if os.path.isdir(os.path.join(test_dir, dirname)):\n",
    "        subtype = dirname.replace('.','/').replace('_',' ')\n",
    "        terms = df_exclude[df_exclude.aspect_subtype == subtype]['term']\n",
    "        print(f'正在处理subtype {subtype}, 找到特征词 {len(terms)}')\n",
    "        result_path= os.path.join(test_dir, dirname, 'exclude.json')\n",
    "        command = f'touch {result_path}'  # 为没有排除词的subtype建立文件进行占位\n",
    "        os.system(command)\n",
    "        for term in terms:\n",
    "            re_exp = term.strip(' ').replace('  ',' ').replace(' ','.{0,20}?')\n",
    "            command = f\"grep '{re_exp}'  {corpus_path} >> {result_path}\"\n",
    "            os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 负样本合并到训练集\n",
    "负样本来源\n",
    "1. 排除词\n",
    "2. 混淆subtype\n",
    "3. 非相关subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T09:39:43.841077Z",
     "start_time": "2019-09-12T09:39:18.177262Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname in os.listdir(test_dir):\n",
    "    if os.path.isdir(os.path.join(test_dir, dirname)):\n",
    "        subtype = dirname.replace('.','/').replace('_',' ')\n",
    "\n",
    "        new_file = os.path.join(test_dir, dirname, 'nega_total.json')\n",
    "        old_file = os.path.join(test_dir, dirname, f'{dirname}.json')\n",
    "        ext_file1 = os.path.join(test_dir, dirname, f'exclude.json')\n",
    "        ext_file2 = os.path.join(test_dir, dirname, f'nega_samples.json')\n",
    "        add_file =  os.path.join(test_dir, dirname, f'nega_add.json')\n",
    "        result_file = os.path.join(test_dir, dirname, f'shuf_{dirname}.json')\n",
    "        \n",
    "        train_file = os.path.join(test_dir, dirname, 'train.json')\n",
    "        dev_file = os.path.join(test_dir, dirname, 'dev.json')\n",
    "\n",
    "        # 负样本略少于正样本10:9\n",
    "        r = !wc -l {old_file}   \n",
    "        posi_num = int(r[0].split()[0])\n",
    "        nega_num = int(posi_num* 0.9)\n",
    "        print('正样本', posi_num, subtype)\n",
    "        \n",
    "        # 如果上面两个来源的负样本数不足,则在进行随机选择负样本补充\n",
    "        r1 = !wc -l {ext_file1}\n",
    "        ext_num1 = int(r1[0].split()[0])\n",
    "        r2 = !wc -l {ext_file2}\n",
    "        ext_num2 = int(r2[0].split()[0])\n",
    "        if ext_num1 + ext_num2 < nega_num:\n",
    "            add_num = nega_num - (ext_num1 + ext_num2)\n",
    "            print(f'{subtype} subtype负样本不足, 补充{add_num}' )\n",
    "            command = f'grep -v \"{subtype}\" {corpus_path} | head -n {add_num*2} > {add_file}'\n",
    "            os.system(command)\n",
    "            command = f\"sort '{ext_file1}' '{ext_file2}' '{add_file}'  |uniq|head -n {nega_num} > {new_file}\"\n",
    "            os.system(command)\n",
    "\n",
    "        else:\n",
    "            command = f\"sort '{ext_file1}' '{ext_file2}' |uniq|head -n {nega_num} > {new_file}\"\n",
    "            os.system(command)\n",
    "\n",
    "        # 按照8:2 拆分训练和测试集\n",
    "        train_num = int((posi_num + nega_num)*0.8)\n",
    "        dev_num = int((posi_num + nega_num)*0.2)\n",
    "        command = f\"sort {new_file} {old_file}| uniq| shuf  > {result_file}\"\n",
    "        os.system(command)\n",
    "        os.system(f'head -n {train_num} {result_file} > {train_file}')\n",
    "        os.system(f'tail -n {dev_num} {result_file} > {dev_file}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T09:39:50.715667Z",
     "start_time": "2019-09-12T09:39:47.111454Z"
    }
   },
   "outputs": [],
   "source": [
    "## 检查训练/测试集 正负样本数, 及时发现异常\n",
    "for dirname in os.listdir(test_dir):\n",
    "    if os.path.isdir(os.path.join(test_dir, dirname)):\n",
    "        subtype = dirname.replace('.','/').replace('_',' ')\n",
    "        \n",
    "        train_file = os.path.join(test_dir, dirname, 'train.json')\n",
    "        dev_file = os.path.join(test_dir, dirname, 'dev.json')\n",
    "\n",
    "        command = f\"grep -v '{subtype}' {train_file}|wc -l\"\n",
    "        r = !{command}\n",
    "        print(r, subtype, '负','train')\n",
    "        command = f\"grep  '{subtype}' {train_file}|wc -l\"\n",
    "        r = !{command}\n",
    "        print(r, subtype, '正','train')\n",
    "        \n",
    "        command = f\"grep -v '{subtype}' {dev_file}|wc -l\"\n",
    "        r = !{command}\n",
    "        print(r, subtype, '负','dev')\n",
    "        command = f\"grep  '{subtype}' {dev_file}|wc -l\"\n",
    "        r = !{command}\n",
    "        print(r, subtype, '正', 'dev')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上, 任务目录下各个subtype文件夹已经生成, train.json和dev.json分别对应了训练和测试集的语料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感极性分类语料生成\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T09:41:41.601519Z",
     "start_time": "2019-09-12T09:41:14.654919Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from utils_ecom_senti import find_positions\n",
    "from data_preprocess import convert_text\n",
    "\n",
    "polar_corpus = []\n",
    "\n",
    "with open(corpus_path, 'r') as cc:\n",
    "    for line in cc:\n",
    "        jl = json.loads(line)\n",
    "        text = jl['text']\n",
    "        clear_text = convert_text(text)\n",
    "        for op in jl['opinions']:\n",
    "            start, end = find_positions(clear_text, op['opinionTerm'])\n",
    "            if start != -2:\n",
    "                subtype = op['aspectSubtype']\n",
    "                polar = op['polarity']\n",
    "                term = clear_text[start:end]\n",
    "                polar_corpus.append((term, clear_text, polar, subtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T09:41:50.289893Z",
     "start_time": "2019-09-12T09:41:50.173371Z"
    }
   },
   "outputs": [],
   "source": [
    "df_polar = pd.DataFrame(polar_corpus ,columns=['opterm','textb','label','subtype'])\n",
    "len(df_polar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T09:42:02.008358Z",
     "start_time": "2019-09-12T09:42:00.496960Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "ratio = 0.8 # 8:2划分训练/测试集\n",
    "corpus_dir = os.path.dirname(corpus_path)\n",
    "for dirname in os.listdir(corpus_dir):\n",
    "    output_dir = os.path.join(corpus_dir, dirname)\n",
    "    subtype = dirname.replace('.', '/').replace('_',\" \")\n",
    "    if subtype not in trans_sub:\n",
    "        continue\n",
    "    print(dirname)\n",
    "    temp  = df_polar[df_polar.subtype == subtype]\n",
    "    temp = temp.sample(frac=1)\n",
    "    train_limit = int(ratio * len(temp))\n",
    "    temp[:train_limit].to_csv(os.path.join(output_dir, 'train.csv'), index=False)\n",
    "    temp[train_limit:].to_csv(os.path.join(output_dir, 'dev.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上, 任务目录下各个subtype文件夹已经生成, train.csv 和dev.csv分别对应了训练和测试集的语料"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
